# SeeingSounds
A music visualiser which makes use of CNN and GAN networks

A system was developed to autonomously generate dynamic visualisation for
any input piece of music. This involves the use of cGAN and CNN networks, as well as
musical feature extraction and image processing algorithms. The high-level goal is to
create new artifacts through modelling the perceptual associations between sound and
image. While a synchronised output was successfully produced, there was no clear perceptual
relation between the input and output, and there was difficulty in reaching network convergence.
Future development could involve expanding the available datasets
and exploring vectorised forms of image representation.
